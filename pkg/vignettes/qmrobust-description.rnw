% -*- mode: noweb; noweb-default-code-mode: R-mode; -*-
%\SweaveUTF8
\documentclass[11pt]{article}
% \VignetteIndexEntry{Package qmrobust for classical and robust quality management}
% \VignettePackage{qmrobust}
% \VignetteDepends{lme4}
% \VignetteDepends{robustlmm}
% \VignetteDepends{outliers}
\usepackage{a4,parskip}
\usepackage{graphicx}
\usepackage{Sweave}
\usepackage{sfsbib}
\usepackage{texab}
\usepackage{/u/stahel/tex/kurs}
%%- \providecommand{\T}{\texttt}
%%- \providecommand{\ul}{\textbf}
%%- \providecommand{\Tit}[1]{\textbf{#1}\hspace{1em}}
%%- \providecommand{\Hneed}[1]{\penalty-8000\hskip#1 plus10pt\penalty-8000\hskip-#1}
%%- \providecommand{\Vneed}[1]{\penalty-5000\vskip#1%\vskip 0pt plus20pt
%%- \penalty-5000\vspace{-#1}}
%%- \providecommand{\vc}[1]{\kern-1pt\underline{\kern1pt#1\kern-1pt}\kern1pt}
%%- \providecommand{\bmath}[1]{\relax\ifmmode{\mbox{\boldmath $#1$}}%
%%-   \else{\boldmath $#1$}\fi}    
%%- \providecommand{\mx}[1]{\bmath#1}  
%%- \providecommand{\sups}[1]{^{(#1)}}
%%- \providecommand{\wb}[1]{{}\if Y#1\overline{Y}\else
%%- \kern0.2em\overline{\kern-0.2em#1}\fi}

\providecommand{\code}[1]{\texttt{#1}}
\def\Cite{\cite}

\setcounter{totalnumber}{5}
\def\textfraction{.0}
\def\floatpagefraction{.1}

\addtolength{\textwidth}{2.5cm}%%--- 15.0 + 2.5 = 17.5
\addtolength{\oddsidemargin}{-5mm}


\begin{document}
\bibliographystyle{sfsbib}      \citationstyle{dcu}
%% ========================================================
%\SweaveOpts{concordance=TRUE}
\SweaveOpts{prefix.string=fig/p-, height=4.5}  
\setkeys{Gin}{width=1\textwidth} %% 0.8 default , height=0.7\textwidth 
%% ========================================================
\title{\vspace*{-10mm}
Package \T{qmrobust} for classical and robust quality management R-functions
}
\author{Werner A. Stahel, ETH Zurich}
\maketitle

\begin{abstract}\noindent
The package \T{qmrobust} presently is a companion to the chapter
``Statistical Procedures for Performance-based Specification and Testing''
of ...

It is planned to implement some more classical and robust methods for 
quantile estimation and interlaboratory studies.
\end{abstract}

<<preliminaries, results=hide>>=
par(ask=FALSE)
par(mar=c(3,3,2,1),mgp=c(2,0.8,0))
@ 
\section{Introduction}
Classical methods for quality management include, besides elementary 
one- and two-sample tests,  
control charts and the analysis of interlaboratory studies.
They are available through existing packages in R.

This vignette is a companion of the chapter
``Statistical Procedures for Performance-based Specification and Testing''
of ...
which presents procedures for quality management in the concrete producing
industry. The package \T{qmrobust} presently consists of the data sets
used in that text and shows how the methods are obtained in R.

<<require,fig=FALSE>>=
require(qmrobust)
@

\section{Datasets}
\Abstit{Datasets \T{d.tunnel1} and \T{d.tunnel2}.}
Concrete properties (compressive strength, permeability and porosity) in
selected structural 
components of a new opencast tunnel have been measured and the spatial
variability determined \Cite{astra}. 
400 cores were taken from two deck and two wall
elements, and different characteristics were measured. 

<<tunnel, fig=FALSE>>=
data(d.tunnel1)
showd(d.tunnel1)
@
<<tunnelhelp, results=hide,fig=FALSE>>=
## ?d.tunnel1 ## help information, not shown here
@
<<tunnelboxplot, fig=TRUE>>=
dd <- d.tunnel1[d.tunnel1$layer=="layer.3",]
plot(strength~section, data=dd, notch=T, ylab="compressive strength [MPa]")
@ 

\Abstit{Dataset \T{d.perm}.}
\label{intro.experm}
The results of a round robin of the air permeability test (SIA Standard
262/1, Annex E) are reported in \Cite{perm09}. 
On two selected elements of a bridge a regular grid of 75 identically 
sized square areas was delineated. The 75 areas were randomly assigned 
to 5 participating teams, so each team had to measure permeability in
15 areas each for both elements.  

<<perm, fig=FALSE>>=
data(d.perm) 
showd(d.perm)
@

%\Abstit{Variables.}

\Abstit{Dataset \T{d.coverdepth}.}
\label{intro.excoverdepth}
A third example is taken from \Cite[Fig.~9.4]{MonAGG14} and features a 
$5\times40$
grid of cover depth readings obtained in the top reinforcement layer of the
deck slab of a freeway viaduct.
%%- Table \ref{tab:coverdepth} shows part of the data. 
%%- 
%%- \Btab{|rrrrrrrr|rrrrrrrr|}
%%- \hline
%%-  64 & 56 & 66 & 76 & 52 & 73 & 69 & 71 & 79 & 76 & 79 & 72 & 76 & 79 & 63 & 53 \\ 
%%-  61 & 56 & 63 & 69 & 69 & 70 & 74 & 76 & 55 & 72 & 76 & 53 & 57 & 76 & 80 & 28 \\ 
%%-  57 & 57 & 37 & 40 & 64 & 56 & 60 & 56 & 56 & 48 & 47 & 57 & 54 & 47 & 56 & 56 \\ 
%%-  49 & 51 & 42 & 51 & 57 & 58 & 57 & 54 & 59 & 63 & 64 & 71 & 48 & 72 & 48 & 39 \\ 
%%-  58 & 61 & 53 & 49 & 51 & 48 & 55 & 50 & 46 & 49 & 65 & 59 & 63 & 62 & 58 &
%%-  47 \\\hline
%%- \Etab{\label{tab:coverdepth}
%%- Cover depth readings for the first 2 of 5 sections of a $5\times 40$ grid
%%- from the top reinforcement layer of the deck slab of a freeway viaduct.}

<<coverdepth, fig=TRUE, height=3.5>>=
data(d.coverdepth)
dd <- d.coverdepth[d.coverdepth$section<=2,]
symbols(dd$column,dd$row, circles=dd$depth, inches=0.7*par("cin")[1],
        xlab="",ylab="")
@ 

%\Abstit{Variables.}

\section{Specification testing}
\Abstit{Inference on a location parameter.}
We assume here that the targetted property of the material, e.g.,
compressive strength,
has a known distribution, usually the normal distribution.
Furthermore, the specification is given as a targeted expected value.
%%- Then, the statistical problem is estimation and testing of the expected
%%- value of a normal distribution. 
Thus, we require inference tools for this parameter.

\Abstit{Classical one-sample t-test and confidence interval.}
The most convenient way to check if a given specification is fulfilled is
provided by a confidence interval for the parameter. 
It is obtained from the function \T{t.test}.

<<ttest>>=
( dd <- d.tunnel1[d.tunnel1$section=="w.1","strength"] )
t.test(dd)
@ 
The estimated strength is $101$ and the confidence interval, $[97.3, 104.1]$. 
If the specification were $100$, it is perfectly fulfilled.

\Abstit{Nonparametric test.}
The assumption of a normal distribution is not needed in this simple
problem, and should be avoided. The corresponding nonparametric test,
the signed rank test of Wilcoxon, provides inference on the center of
symmetry of the distribution.

<<wilctest>>=
dd <- d.tunnel1[d.tunnel1$section=="w.1","strength"]
wilcox.test(dd, conf.int=T)
@ 
The center of symmetry is estimated as 
\Sexpr{round((rr <- wilcox.test(dd,conf.int=TRUE))$estimate,2)}, 
with a confidence interval from 
\Sexpr{round(rr$conf.int[1],2)} to \Sexpr{round(rr$conf.int[2],2)}.

\Abstit{Outlier rejection.}
The most popular tests for outliers is Grubbs' tests.
They are all based on the sorted, standardized observations $X_{[i]}$,
$R_i=(X_{[i]}-\wb X)/S$, where $S$ is the estimated standard deviation.

The first test is intended to detect single outliers at either the low or
the high end of the sample. 
It is based on the test statistic
$T_1=\max\fn{-R_1,R_n}$.
The second test is designed to detect the case of two outliers on the 
same side. The test statistic is 
$T_2=\max\fn{R_1^2+R_2^2, R_{n-1}^2+R_n^2}$.
The third test should pinpoint two outliers, one at each end, by
calculating $T_3=R_n-R_1$.
The distribution of these test statistics has been derived in the
literature. 

The package \code{outliers} contains a function 
\code{grubbs.test} which performs three types of this test.
A peculiar feature of the function is its default value \code{FALSE}
for the argument \code{two.sided}. Because the side on which the outliers
can occur is rarely known a priori, the argument should be set \code{TRUE},
except for \code{type=11}, for which this setting produces nonsense.

<<grubbs>>=
require(outliers)
dd <- d.tunnel1[d.tunnel1$section=="w.1","strength"][1:10]
grubbs.test(dd, type=10, two.sided=TRUE) ## one outlier on either side?
grubbs.test(c(dd,60), type=10, two.sided=TRUE)
grubbs.test(c(dd,60), type=11)
grubbs.test(dd, type=20, two.sided=TRUE)
grubbs.test(c(dd[1:10],60), type=20, two.sided=TRUE)
@ 

%% \Abstit{Robust testing.}

\section{Conformity testing.}
\Abstit{Quantile estimation.}
Quality measures have a natural variability across the object (brigde,
tunnel, ...) to be assessed. Therefore, it will not be sufficient to
specify a requirement in terms of the expected value. 
Rather, a ``reasonable minimum'' should be prescribed, in the form:
``The quality criterion $X$ shall be larger than $c$ with a probability
$1-\gamma$, i.e., $P(X<c)<\gamma$.
Equivalently, the $\gamma$ quantile $q_\gamma$ should be $\ge c$.

Since the decision whether confirmity is warranted or not must be based on
a sample, the random nature of the estimators of $P(X<c)$ or $q_\gamma$ 
should be taken into account as discussed below.

\Abstit{Nonparametric inference.}
The most direct way to assess $P(X<c)$ is based on the number $K$ of
observations for which $X_i<c$. It has a binomial distribution,
$K\sim\B\fn{n,p}$.
Conformity testing either means testing $p\ge\gamma$ against $p<\gamma$ or
determining a one-sided confidence interval for $p$ and checking if
the upper bound is $\le\gamma$.  
We do both for the cover depth data and a threshold of $c=45$mm,
which is required to be failed with less than $\gamma=10\%$ probability.

<<conformitybinom>>=
##- hist(d.coverdepth$depth)  # recommended, but not done here.
threshold <- 45
gamma <- 0.1
x <- sum(d.coverdepth$depth < threshold)
binom.test( x, nrow(d.coverdepth), p=gamma, alternative="less")
@
<<conformitybin2, echo=FALSE>>=
t.t <- binom.test( x, nrow(d.coverdepth), p=gamma, alternative="less")
@ 

Thus, with $n=\Sexpr{(t.n <- nrow(d.coverdepth))}$ observations, the
conformity test for $c=40$mm was successful, since the p-value is 
$\Sexpr{format(t.t$p.value,digits=3)}<\gamma$,
or since the upper bound of the confidence interval,
\Sexpr{round(t.t$conf.int[2],4)}, is $<\gamma$.
The estimated probability of a lower cover depth was $\wh p=\Sexpr{x/t.n}$.

Such a large sample may be realistic for easy, non-destructive
measurements. For other situations, an alternative assessment of conformity
is needed.

\Abstit{Quantile estimation for normal data with known scale.}
If the normal distribution $\N\fn{\mu,\sigma^2}$
is assumed for the data, then the quantile equals
$q_\gamma = \mu+q_\gamma\sups{0,1}\sigma$, where 
$q_\gamma\sups{0,1}$ is the $\gamma$ quantile of the standard normal
distribution.
If the precision of the measurements, expressed by $\sigma$, is known --
possibly from many similar studies --, then this leads to the estimated
quantile $\wh q_\gamma = \wb x+q_\gamma\sups{0,1}\sigma$.
For the data used before and $\sigma=15$MPa,
<<quantile>>=
dd <- d.tunnel1[d.tunnel1$section=="w.1","strength"]
gamma <- 0.02
sigma <- 15
mean(dd, na.rm=TRUE) + qnorm(gamma)*sigma
@ 
For drawing inference, the easiest way is to check if the expected value
$\mu$ is larger than $c+q_\gamma\sups{0,1}\sigma$ as discussed above.

\Abstit{Normal data with unknown scale.} ???
%%- Since this quantity is itself random, one should make sure that the
%%- condition is satisfied with a confidence level of $1-\alpha$, that is,
%%- $\wh q_\gamma-q_\alpha\sups{0,1}\se_{q_\gamma}\ge c$.
%%- 
%%- How large is the standard error $\se_{q_\gamma}$ of the estimated $q_\gamma$?
%%- We know that $\sd_{\wb x}=\sigma/\sqrt n$. 
%%- The standard error of $\wh\sigma$ can be derived from
%%- $\var\fn{\wh\sigma^2} = $ to be $...$.
%%- Thus, $\se_{q_\gamma}=...$.
%%- 

%% \Abstit{Robust quantile estimation.}

\section{Interlaboratory studies}
\Abstit{Standard procedure.}
The classical analysis of interlaboratory studies is based on a one-way
analysis of variance. Outlier tests are usually applied to eliminate
extreme observations within a lab or the entire set from a lab
which deviates extremely from the other labs.

The package provides the function \T{interlabstats}, which does not (yet)
implement any outlier testing or rejection. It comes with 
\T{print} and \T{plot} methods.

For obtaining an instructive example, the data for team E in \T{d.perm}
have been changed, 
because otherwise, the between groups variance is estimated as zero, which
is an atypical case. 

<<interlabstats, fig=TRUE>>=
dd <- d.perm[d.perm$section=="w.1",]
dd[dd$team=="E","perm.log"] <- dd[dd$team=="E","perm.log"]+0.5
( r.cl <- interlabstats(perm.log~team, data=dd) )
plot(r.cl)
@ 

\Abstit{Mixed model analysis.}
A more modern way of analyzing interlaboratory studies relies on maximum
likelihood estimation of the parameters of the model. 
This is achieved by the R-function \T{lmer} of the package \T{lme4}.
The results can be fed into \code{interlabstats} to get the desired
quantities. 
<<interlablme>>=
require(lme4)
r.lme <- lmer(perm.log~(1|team), data=dd, na.action=na.omit)
summary(r.lme)
## ---------------------
interlabstats(r.lme)
@ 


\Abstit{Robust mixed model analysis.}
The classical norms based on the rejection of outlying observations and
labs lead to biased results.
(Corrections would in principle be possible but are not worked out
according to the knowledge of the authors.)
An alternative is to use robust estimators, which avoid the rejection of
data, but deal with outliers by downweighting their influence on the
results adequately.
A robust version of the maximum likelihood method is obtained from the
package \code{robustlmm}, function \code{rlmer}.
<<interlabroblme>>=
# require(robustlmm)
# r.lmmrob <- rlmer(perm.log~(1|team), data=dd) 
# save(r.lmmrob,file="r.lmmrob")
load("r.lmmrob")
## summary(r.lmmrob)
interlabstats(r.lmmrob)
@ 
In our example, the robust and nonrobust mixed model estimators are quite 
similar but different from the classical one:

\section{Regression}
\Abstit{Fitting the model.}
Linear regression models are fitted by the R function \code{lm}.
<<regression, fig=TRUE, height=5>>=
r.lm <- lm(log10(strength) ~ log10(density), data=d.tunnel1)
options(show.signif.stars=FALSE)
summary(r.lm) ## results of the fitting
plot(log10(strength) ~ log10(density), data=d.tunnel1)  ## scater plot
abline(r.lm)  ## draw the estimated regression line
@ 

\Abstit{Residual analysis.}

<<resanal, fig=TRUE>>=
par(mfrow=c(2,2), mar=c(3,3,2,1), mgp=c(2,0.8,0))
plot(r.lm)
@ 

\Abstit{Prediction.}

<<predict, fig=TRUE, height=5>>=
plot(log10(strength) ~ log10(density), data=d.tunnel1)  ## scater plot
abline(r.lm)  ## draw the estimated regression line

lusr <- par("usr")
x <- seq(lusr[1],lusr[2],length=51) ## equally space values along x
xdf <- data.frame(density=10^x)
r.pred <- predict(r.lm, newdata=xdf, interval="prediction")
matlines(x,r.pred[,2:3], lty=5, col="red", lwd=2)
@ 

\bibliography{/u/stahel/proj/statintro/concrete.bib}

\end{document}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 
